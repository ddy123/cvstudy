import cv2
import numpy as np
from ultralytics import YOLO
import time
from collections import defaultdict
import os

model_path = os.path.expanduser("/home/ddy/code/python/cvstudy/yolov8m.pt")
model = YOLO(model_path)

class YOLOScentDetector:
    def __init__(self, model_size='m'):
        """
        åˆå§‹åŒ–YOLOç‰©ä½“æ£€æµ‹å™¨
        model_size: n/s/m/l/x å¯¹åº”ä¸åŒæ¨¡å‹å¤§å°
        """
        # åŠ è½½é¢„è®­ç»ƒYOLOæ¨¡å‹
        self.model = YOLO(f'yolov8{model_size}.pt')
        #self.model = YOLO(model)
        
        # ç‰©ä½“åˆ°æ°”å‘³çš„æ˜ å°„
        self.scent_mapping = {
            # æ¤ç‰©ç›¸å…³
            'potted plant': ['æ¤ç‰©æ¸…é¦™', 'ç»¿å¶æ°”æ¯'],
            'vase': ['èŠ±é¦™', 'æ¤ç‰©èŠ³é¦™'],
            
            # é£Ÿç‰©æ°´æœ
            'apple': ['è‹¹æœé¦™', 'æœé¦™'],
            'orange': ['æ©™å­é¦™', 'æŸ‘æ©˜è°ƒ'],
            'banana': ['é¦™è•‰å‘³', 'ç”œé¦™'],
            'sandwich': ['é¢åŒ…é¦™', 'é£Ÿææ··åˆé¦™'],
            'pizza': ['çƒ˜ç„™é¦™', 'å¥¶é…ªé¦™'],
            'cake': ['ç”œç‚¹é¦™', 'ç³–éœœé¦™'],
            
            # é¥®å“ç›¸å…³
            'wine glass': ['è‘¡è„é…’é¦™', 'æœé…’æ°”æ¯'],
            'cup': ['é¥®å“é¦™æ°”', 'çƒ­é¥®é¦™'],
            'bottle': ['ç“¶ä¸­ç‰©æ°”å‘³', 'æ¶²ä½“é¦™æ°”'],
            
            # å…¶ä»–
            'book': ['ä¹¦é¦™', 'çº¸å¼ å‘³'],
            'candle': ['èœ¡é¦™', 'ç‡ƒçƒ§æ°”æ¯']
        }
        
        # æ£€æµ‹å†å²è®°å½•
        self.detection_history = defaultdict(list)
        self.frame_count = 0
        
    def detect_objects(self, frame, confidence_threshold=0.5):
        """
        ä½¿ç”¨YOLOæ£€æµ‹ç‰©ä½“
        """
        # è¿è¡ŒYOLOæ£€æµ‹
        results = self.model(frame, verbose=False)
        
        detected_objects = []
        
        for result in results:
            for box in result.boxes:
                conf = float(box.conf[0])
                if conf > confidence_threshold:
                    class_id = int(box.cls[0])
                    class_name = result.names[class_id]
                    bbox = box.xyxy[0].cpu().numpy()
                    
                    detected_objects.append({
                        'class_name': class_name,
                        'confidence': conf,
                        'bbox': bbox,
                        'scent': self.scent_mapping.get(class_name, [])
                    })
        
        return detected_objects
    
    def draw_detections(self, frame, detections):
        """
        åœ¨å¸§ä¸Šç»˜åˆ¶æ£€æµ‹ç»“æœ
        """
        for detection in detections:
            x1, y1, x2, y2 = detection['bbox'].astype(int)
            class_name = detection['class_name']
            confidence = detection['confidence']
            
            # ç»˜åˆ¶è¾¹ç•Œæ¡†
            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)
            
            # ç»˜åˆ¶æ ‡ç­¾èƒŒæ™¯
            label = f"{class_name} {confidence:.2f}"
            label_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 2)[0]
            cv2.rectangle(frame, (x1, y1 - label_size[1] - 10), 
                         (x1 + label_size[0], y1), (0, 255, 0), -1)
            
            # ç»˜åˆ¶æ ‡ç­¾æ–‡æœ¬
            cv2.putText(frame, label, (x1, y1 - 5), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 2)
            
            # å¦‚æœæœ‰æ°”å‘³ä¿¡æ¯ï¼Œæ˜¾ç¤ºåœ¨æ¡†ä¸Šæ–¹
            if detection['scent']:
                scent_text = f"Scent: {', '.join(detection['scent'])}"
                cv2.putText(frame, scent_text, (x1, y1 - 25), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 255, 0), 1)
        
        return frame
    
    def process_video_realtime(self, video_source=0, output_file=None):
        """
        å®æ—¶å¤„ç†è§†é¢‘å¹¶æ£€æµ‹ç‰©ä½“å’Œæ°”å‘³
        """
        # æ‰“å¼€è§†é¢‘æº
        cap = cv2.VideoCapture(video_source)
        if not cap.isOpened():
            print("âŒ æ— æ³•æ‰“å¼€è§†é¢‘æº")
            return
        
        # è·å–è§†é¢‘å±æ€§
        fps = cap.get(cv2.CAP_PROP_FPS)
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        
        # è®¾ç½®è§†é¢‘å†™å…¥å™¨ï¼ˆå¦‚æœéœ€è¦ä¿å­˜ç»“æœï¼‰
        if output_file:
            fourcc = cv2.VideoWriter_fourcc(*'XVID')
            out = cv2.VideoWriter(output_file, fourcc, fps, (width, height))
        
        print("ğŸ¬ å¼€å§‹å®æ—¶ç‰©ä½“æ£€æµ‹...")
        print("æŒ‰ 'q' é€€å‡ºï¼ŒæŒ‰ 'p' æš‚åœ")
        
        paused = False
        start_time = time.time()
        frame_count = 0
        
        while True:
            if not paused:
                ret, frame = cap.read()
                if not ret:
                    break
                
                frame_count += 1
                
                # ç‰©ä½“æ£€æµ‹
                detection_start = time.time()
                detections = self.detect_objects(frame)
                detection_time = time.time() - detection_start
                
                # ç»˜åˆ¶æ£€æµ‹ç»“æœ
                frame_with_detections = self.draw_detections(frame.copy(), detections)
                
                # æ·»åŠ ä¿¡æ¯é¢æ¿
                frame_with_detections = self.add_info_panel(
                    frame_with_detections, detections, detection_time, frame_count
                )
                
                # æ˜¾ç¤ºç»“æœ
                cv2.imshow('YOLOç‰©ä½“æ£€æµ‹ - æ°”å‘³è¯†åˆ«', frame_with_detections)
                
                # ä¿å­˜ç»“æœï¼ˆå¦‚æœéœ€è¦ï¼‰
                if output_file:
                    out.write(frame_with_detections)
            
            # é”®ç›˜æ§åˆ¶
            key = cv2.waitKey(1) & 0xFF
            if key == ord('q'):
                break
            elif key == ord('p'):
                paused = not paused
                print("â¸ï¸ æš‚åœ" if paused else "â–¶ï¸ ç»§ç»­")
        
        # æ¸…ç†èµ„æº
        cap.release()
        if output_file:
            out.release()
        cv2.destroyAllWindows()
        
        # æ€§èƒ½ç»Ÿè®¡
        total_time = time.time() - start_time
        avg_fps = frame_count / total_time
        print(f"\nğŸ“Š æ€§èƒ½ç»Ÿè®¡:")
        print(f"æ€»å¸§æ•°: {frame_count}")
        print(f"æ€»æ—¶é—´: {total_time:.2f}s")
        print(f"å¹³å‡FPS: {avg_fps:.2f}")
    
    def add_info_panel(self, frame, detections, detection_time, frame_count):
        """
        åœ¨å¸§ä¸Šæ·»åŠ ä¿¡æ¯é¢æ¿
        """
        # åŸºæœ¬ä¿¡æ¯
        cv2.putText(frame, f"Frame: {frame_count}", (10, 30), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
        cv2.putText(frame, f"Detection Time: {detection_time*1000:.1f}ms", (10, 60), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)
        cv2.putText(frame, f"Objects Detected: {len(detections)}", (10, 90), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 0), 2)
        
        # æ£€æµ‹åˆ°çš„æ°”å‘³
        current_scents = set()
        for detection in detections:
            current_scents.update(detection['scent'])
        
        if current_scents:
            scent_text = "Detected Scents: " + ", ".join(current_scents)
            cv2.putText(frame, scent_text, (10, 120), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
        else:
            cv2.putText(frame, "No scents detected", (10, 120), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 0, 0), 2)
        
        return frame
    
    def analyze_video_scents(self, video_path, frame_interval=10):
        """
        åˆ†æè§†é¢‘ä¸­çš„æ°”å‘³å‡ºç°æ¨¡å¼
        """
        cap = cv2.VideoCapture(video_path)
        fps = cap.get(cv2.CAP_PROP_FPS)
        
        scent_timeline = []
        frame_count = 0
        
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            
            if frame_count % frame_interval == 0:
                timestamp = frame_count / fps
                
                # æ£€æµ‹ç‰©ä½“
                detections = self.detect_objects(frame)
                
                # æ”¶é›†æ°”å‘³ä¿¡æ¯
                current_scents = set()
                for detection in detections:
                    current_scents.update(detection['scent'])
                
                if current_scents:
                    scent_timeline.append({
                        'timestamp': timestamp,
                        'scents': list(current_scents),
                        'objects': [d['class_name'] for d in detections]
                    })
            
            frame_count += 1
        
        cap.release()
        return scent_timeline

# ğŸš€ ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # åˆ›å»ºæ£€æµ‹å™¨
    detector = YOLOScentDetector(model_size='m')
    
    # æ–¹æ³•1: å®æ—¶æ‘„åƒå¤´æ£€æµ‹
    #print("ğŸ“· å¯åŠ¨æ‘„åƒå¤´å®æ—¶æ£€æµ‹...")
    #detector.process_video_realtime(0)  # 0 è¡¨ç¤ºé»˜è®¤æ‘„åƒå¤´
    
    # æ–¹æ³•2: å¤„ç†è§†é¢‘æ–‡ä»¶
    detector.process_video_realtime("/home/ddy/code/python/cvstudy/input.mp4", "/home/ddy/code/python/cvstudy/output_frames")
    
    # æ–¹æ³•3: åˆ†æè§†é¢‘æ°”å‘³æ—¶é—´çº¿
    # timeline = detector.analyze_video_scents('input_video.mp4')
    # for entry in timeline:
    #     print(f"Time: {entry['timestamp']:.1f}s - Scents: {entry['scents']}")