#!/usr/bin/env python3
"""
å¤šæ¨¡æ€RAGç³»ç»Ÿç¤ºä¾‹
ç»“åˆæ–‡æœ¬RAGå’ŒLLaVAå›¾åƒç†è§£
"""
import os
import time
import base64
import requests
from PIL import Image
import io
from typing import List, Dict, Any, Optional
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.schema import Document
import torch

class MultimodalRAGSystem:
    def __init__(self, 
                 rag_device: str = "cuda:0",
                 llava_device: str = "cuda:1"):
        """
        åˆå§‹åŒ–å¤šæ¨¡æ€RAGç³»ç»Ÿ
        
        Args:
            rag_device: RAGç³»ç»Ÿä½¿ç”¨çš„GPU
            llava_device: LLaVAç³»ç»Ÿä½¿ç”¨çš„GPU
        """
        print("åˆå§‹åŒ–å¤šæ¨¡æ€RAGç³»ç»Ÿ...")
        
        # 1. åˆå§‹åŒ–æ–‡æœ¬RAGç³»ç»Ÿ
        print("åˆå§‹åŒ–æ–‡æœ¬RAGç³»ç»Ÿ...")
        self.rag_device = rag_device
        os.environ["CUDA_VISIBLE_DEVICES"] = "0"  # ç¬¬ä¸€å¼ GPUç»™RAG
        
        self.embeddings = HuggingFaceEmbeddings(
            model_name="BAAI/bge-large-zh-v1.5",
            model_kwargs={'device': rag_device},
            encode_kwargs={'normalize_embeddings': True}
        )
        
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=512,
            chunk_overlap=50
        )
        
        self.vector_store = None
        self.index_path = "faiss_index"
        
        # 2. åˆå§‹åŒ–LLaVAç³»ç»Ÿ
        print("åˆå§‹åŒ–LLaVAç³»ç»Ÿ...")
        os.environ["CUDA_VISIBLE_DEVICES"] = "1"  # ç¬¬äºŒå¼ GPUç»™LLaVA
        
        # è¿™é‡Œæˆ‘ä»¬å‡è®¾LLaVAé€šè¿‡APIæœåŠ¡è¿è¡Œ
        self.llava_api_url = "http://localhost:8000/generate"
        self.llava_device = llava_device
        
        # æˆ–è€…ç›´æ¥åŠ è½½æ¨¡å‹ï¼ˆå¦‚æœå†…å­˜å…è®¸ï¼‰
        self.llava_model = None
        self.llava_processor = None
        
        # å°è¯•ç›´æ¥åŠ è½½LLaVAæ¨¡å‹
        self._load_llava_model()
        
        print("å¤šæ¨¡æ€RAGç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
    
    def _load_llava_model(self):
        """
        åŠ è½½LLaVAæ¨¡å‹
        """
        try:
            from transformers import AutoProcessor, LlavaForConditionalGeneration
            
            print("åŠ è½½LLaVAæ¨¡å‹...")
            model_id = "llava-hf/llava-1.6-vicuna-7b-hf"  # è¾ƒå°çš„æ¨¡å‹
            
            self.llava_processor = AutoProcessor.from_pretrained(model_id)
            self.llava_model = LlavaForConditionalGeneration.from_pretrained(
                model_id,
                torch_dtype=torch.float16,
                device_map=self.llava_device
            )
            
            print("LLaVAæ¨¡å‹åŠ è½½æˆåŠŸ")
            
        except Exception as e:
            print(f"LLaVAæ¨¡å‹åŠ è½½å¤±è´¥ï¼Œå°†ä½¿ç”¨APIæ¨¡å¼: {e}")
            self.llava_model = None
    
    def create_text_index(self, documents: List[Dict[str, Any]]):
        """
        åˆ›å»ºæ–‡æœ¬å‘é‡ç´¢å¼•
        
        Args:
            documents: æ–‡æ¡£åˆ—è¡¨
        """
        print("åˆ›å»ºæ–‡æœ¬å‘é‡ç´¢å¼•...")
        
        # è½¬æ¢ä¸ºLangChainæ–‡æ¡£
        langchain_docs = []
        for doc in documents:
            langchain_docs.append(Document(
                page_content=doc['content'],
                metadata=doc.get('metadata', {})
            ))
        
        # åˆ†å‰²æ–‡æ¡£
        splits = self.text_splitter.split_documents(langchain_docs)
        print(f"æ–‡æ¡£åˆ†å‰²ä¸º {len(splits)} ä¸ªå—")
        
        # åˆ›å»ºå‘é‡å­˜å‚¨
        self.vector_store = FAISS.from_documents(
            documents=splits,
            embedding=self.embeddings
        )
        
        # ä¿å­˜ç´¢å¼•
        self.vector_store.save_local(self.index_path)
        print(f"ç´¢å¼•å·²ä¿å­˜åˆ°: {self.index_path}")
    
    def load_text_index(self):
        """
        åŠ è½½æ–‡æœ¬å‘é‡ç´¢å¼•
        """
        if os.path.exists(self.index_path):
            print(f"åŠ è½½æ–‡æœ¬å‘é‡ç´¢å¼•: {self.index_path}")
            self.vector_store = FAISS.load_local(
                self.index_path,
                self.embeddings,
                allow_dangerous_deserialization=True
            )
            print("æ–‡æœ¬ç´¢å¼•åŠ è½½æˆåŠŸ")
        else:
            print("æœªæ‰¾åˆ°æ–‡æœ¬ç´¢å¼•æ–‡ä»¶")
    
    def text_search(self, query: str, k: int = 5) -> List[Dict[str, Any]]:
        """
        æ–‡æœ¬æœç´¢
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            k: è¿”å›ç»“æœæ•°é‡
            
        Returns:
            æœç´¢ç»“æœ
        """
        if self.vector_store is None:
            print("è¯·å…ˆåŠ è½½æ–‡æœ¬ç´¢å¼•")
            return []
        
        results = []
        docs = self.vector_store.similarity_search(query, k=k)
        
        for i, doc in enumerate(docs):
            results.append({
                'rank': i + 1,
                'content': doc.page_content[:200] + "..." if len(doc.page_content) > 200 else doc.page_content,
                'metadata': doc.metadata
            })
        
        return results
    
    def process_image_local(self, image_path: str, question: str) -> str:
        """
        ä½¿ç”¨æœ¬åœ°LLaVAæ¨¡å‹å¤„ç†å›¾åƒ
        
        Args:
            image_path: å›¾åƒè·¯å¾„
            question: é—®é¢˜
            
        Returns:
            å›ç­”
        """
        if self.llava_model is None or self.llava_processor is None:
            return "LLaVAæ¨¡å‹æœªåŠ è½½"
        
        try:
            # åŠ è½½å›¾åƒ
            image = Image.open(image_path).convert("RGB")
            
            # å‡†å¤‡è¾“å…¥
            prompt = f"USER: <image>\n{question}\nASSISTANT:"
            
            inputs = self.llava_processor(
                text=prompt,
                images=image,
                return_tensors="pt"
            ).to(self.llava_device)
            
            # ç”Ÿæˆå›ç­”
            with torch.no_grad():
                output = self.llava_model.generate(
                    **inputs,
                    max_new_tokens=200,
                    do_sample=False
                )
            
            # è§£ç è¾“å‡º
            response = self.llava_processor.decode(
                output[0], 
                skip_special_tokens=True
            )
            
            # æå–åŠ©æ‰‹å›ç­”
            if "ASSISTANT:" in response:
                response = response.split("ASSISTANT:")[-1].strip()
            
            return response
            
        except Exception as e:
            return f"å›¾åƒå¤„ç†å¤±è´¥: {str(e)}"
    
    def process_image_api(self, image_path: str, question: str) -> str:
        """
        é€šè¿‡APIå¤„ç†å›¾åƒ
        
        Args:
            image_path: å›¾åƒè·¯å¾„
            question: é—®é¢˜
            
        Returns:
            å›ç­”
        """
        try:
            # è¯»å–å›¾åƒå¹¶ç¼–ç ä¸ºbase64
            with open(image_path, "rb") as f:
                image_data = base64.b64encode(f.read()).decode('utf-8')
            
            # å‡†å¤‡è¯·æ±‚æ•°æ®
            payload = {
                "image": image_data,
                "prompt": question,
                "max_tokens": 200
            }
            
            # å‘é€è¯·æ±‚
            response = requests.post(
                self.llava_api_url,
                json=payload,
                timeout=30
            )
            
            if response.status_code == 200:
                result = response.json()
                return result.get("response", "æœªæ”¶åˆ°æœ‰æ•ˆå“åº”")
            else:
                return f"APIè¯·æ±‚å¤±è´¥: {response.status_code}"
                
        except Exception as e:
            return f"APIè°ƒç”¨å¤±è´¥: {str(e)}"
    
    def process_image(self, image_path: str, question: str) -> str:
        """
        å¤„ç†å›¾åƒï¼ˆè‡ªåŠ¨é€‰æ‹©æœ¬åœ°æˆ–APIæ¨¡å¼ï¼‰
        
        Args:
            image_path: å›¾åƒè·¯å¾„
            question: é—®é¢˜
            
        Returns:
            å›ç­”
        """
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(image_path):
            return f"å›¾åƒæ–‡ä»¶ä¸å­˜åœ¨: {image_path}"
        
        # ä¼˜å…ˆä½¿ç”¨æœ¬åœ°æ¨¡å‹
        if self.llava_model is not None:
            print("ä½¿ç”¨æœ¬åœ°LLaVAæ¨¡å‹å¤„ç†å›¾åƒ")
            return self.process_image_local(image_path, question)
        else:
            print("ä½¿ç”¨APIæ¨¡å¼å¤„ç†å›¾åƒ")
            return self.process_image_api(image_path, question)
    
    def multimodal_query(self, 
                        text_query: str, 
                        image_path: Optional[str] = None) -> Dict[str, Any]:
        """
        å¤šæ¨¡æ€æŸ¥è¯¢
        
        Args:
            text_query: æ–‡æœ¬æŸ¥è¯¢
            image_path: å›¾åƒè·¯å¾„ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            æŸ¥è¯¢ç»“æœ
        """
        result = {
            "text_query": text_query,
            "image_path": image_path,
            "text_results": [],
            "image_response": None,
            "combined_response": None
        }
        
        # 1. æ–‡æœ¬æœç´¢
        print("æ‰§è¡Œæ–‡æœ¬æœç´¢...")
        text_results = self.text_search(text_query, k=3)
        result["text_results"] = text_results
        
        # 2. å›¾åƒå¤„ç†ï¼ˆå¦‚æœæä¾›äº†å›¾åƒï¼‰
        if image_path and os.path.exists(image_path):
            print("å¤„ç†å›¾åƒ...")
            image_response = self.process_image(image_path, text_query)
            result["image_response"] = image_response
            
            # 3. ç»¼åˆå“åº”
            print("ç”Ÿæˆç»¼åˆå“åº”...")
            combined_response = self._combine_responses(text_results, image_response, text_query)
            result["combined_response"] = combined_response
        
        return result
    
    def _combine_responses(self, 
                          text_results: List[Dict[str, Any]], 
                          image_response: str,
                          query: str) -> str:
        """
        ç»¼åˆæ–‡æœ¬å’Œå›¾åƒå“åº”
        
        Args:
            text_results: æ–‡æœ¬æœç´¢ç»“æœ
            image_response: å›¾åƒå¤„ç†ç»“æœ
            query: åŸå§‹æŸ¥è¯¢
            
        Returns:
            ç»¼åˆå“åº”
        """
        # æ„å»ºç»¼åˆå“åº”
        response = f"é’ˆå¯¹æ‚¨çš„é—®é¢˜ '{query}'ï¼Œä»¥ä¸‹æ˜¯ç»¼åˆåˆ†æï¼š\n\n"
        
        # æ·»åŠ æ–‡æœ¬åˆ†æ
        if text_results:
            response += "ğŸ“š åŸºäºæ–‡æ¡£åˆ†æï¼š\n"
            for i, result in enumerate(text_results):
                response += f"{i+1}. {result['content']}\n"
            response += "\n"
        
        # æ·»åŠ å›¾åƒåˆ†æ
        if image_response and image_response != "LLaVAæ¨¡å‹æœªåŠ è½½":
            response += "ğŸ–¼ï¸ åŸºäºå›¾åƒåˆ†æï¼š\n"
            response += f"{image_response}\n\n"
        
        # æ€»ç»“
        response += "ğŸ’¡ æ€»ç»“ï¼š"
        if text_results and image_response:
            response += "ç»“åˆæ–‡æœ¬èµ„æ–™å’Œå›¾åƒå†…å®¹ï¼Œ"
        elif text_results:
            response += "æ ¹æ®æ–‡æœ¬èµ„æ–™ï¼Œ"
        elif image_response:
            response += "æ ¹æ®å›¾åƒåˆ†æï¼Œ"
        
        response += "ä»¥ä¸Šæ˜¯ç›¸å…³ä¿¡æ¯ã€‚"
        
        return response

def create_sample_data():
    """
    åˆ›å»ºç¤ºä¾‹æ•°æ®
    """
    # åˆ›å»ºç¤ºä¾‹æ–‡æœ¬æ–‡ä»¶
    text_file = "sample_tech.txt"
    with open(text_file, "w", encoding="utf-8") as f:
        f.write("""è®¡ç®—æœºè§†è§‰æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œä½¿è®¡ç®—æœºèƒ½å¤Ÿä»æ•°å­—å›¾åƒæˆ–è§†é¢‘ä¸­è·å–é«˜å±‚æ¬¡çš„ç†è§£ã€‚

æ·±åº¦å­¦ä¹ æ¨¡å‹å¦‚å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰åœ¨å›¾åƒåˆ†ç±»ã€ç‰©ä½“æ£€æµ‹å’Œå›¾åƒåˆ†å‰²ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚

OpenCVæ˜¯ä¸€ä¸ªå¼€æºçš„è®¡ç®—æœºè§†è§‰åº“ï¼ŒåŒ…å«æ•°ç™¾ç§è®¡ç®—æœºè§†è§‰ç®—æ³•ã€‚

å›¾åƒå¤„ç†æŠ€æœ¯åŒ…æ‹¬å›¾åƒå¢å¼ºã€æ»¤æ³¢ã€è¾¹ç¼˜æ£€æµ‹å’Œå½¢æ€å­¦æ“ä½œã€‚

è®¡ç®—æœºè§†è§‰åº”ç”¨åŒ…æ‹¬äººè„¸è¯†åˆ«ã€è‡ªåŠ¨é©¾é©¶æ±½è½¦ã€åŒ»å­¦å›¾åƒåˆ†æå’Œå·¥ä¸šæ£€æµ‹ã€‚""")
    
    # åˆ›å»ºç¤ºä¾‹å›¾åƒï¼ˆå¦‚æœéœ€è¦ï¼Œå¯ä»¥ä¸‹è½½ä¸€ä¸ªç¤ºä¾‹å›¾åƒï¼‰
    # è¿™é‡Œæˆ‘ä»¬åˆ›å»ºä¸€ä¸ªç®€å•çš„PILå›¾åƒä½œä¸ºç¤ºä¾‹
    image_file = "sample_image.png"
    img = Image.new('RGB', (300, 200), color='blue')
    
    # åœ¨å›¾åƒä¸Šæ·»åŠ ä¸€äº›æ–‡æœ¬
    from PIL import ImageDraw, ImageFont
    draw = ImageDraw.Draw(img)
    
    # ä½¿ç”¨é»˜è®¤å­—ä½“
    try:
        font = ImageFont.truetype("arial.ttf", 20)
    except:
        font = ImageFont.load_default()
    
    draw.text((50, 80), "ç¤ºä¾‹å›¾åƒ", fill="white", font=font)
    draw.text((50, 110), "è®¡ç®—æœºè§†è§‰ç¤ºä¾‹", fill="yellow", font=font)
    
    img.save(image_file)
    
    return text_file, image_file

def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("å¤šæ¨¡æ€RAGç³»ç»Ÿç¤ºä¾‹")
    print("=" * 60)
    
    # 1. åˆå§‹åŒ–ç³»ç»Ÿ
    print("\nåˆå§‹åŒ–å¤šæ¨¡æ€RAGç³»ç»Ÿ...")
    multimodal_system = MultimodalRAGSystem(
        rag_device="cuda:0",
        llava_device="cuda:1"
    )
    
    # 2. åˆ›å»ºç¤ºä¾‹æ•°æ®
    print("\nåˆ›å»ºç¤ºä¾‹æ•°æ®...")
    text_file, image_file = create_sample_data()
    
    # 3. åŠ è½½æ–‡æœ¬æ•°æ®å¹¶åˆ›å»ºç´¢å¼•
    print("\nåŠ è½½æ–‡æœ¬æ•°æ®...")
    with open(text_file, "r", encoding="utf-8") as f:
        content = f.read()
    
    documents = [{
        'content': content,
        'metadata': {'source': text_file, 'type': 'æŠ€æœ¯æ–‡æ¡£'}
    }]
    
    multimodal_system.create_text_index(documents)
    
    # 4. æµ‹è¯•æ–‡æœ¬æœç´¢
    print("\n" + "=" * 60)
    print("æµ‹è¯•æ–‡æœ¬æœç´¢")
    print("=" * 60)
    
    text_queries = [
        "ä»€ä¹ˆæ˜¯è®¡ç®—æœºè§†è§‰ï¼Ÿ",
        "æ·±åº¦å­¦ä¹ åœ¨è®¡ç®—æœºè§†è§‰ä¸­æœ‰ä»€ä¹ˆåº”ç”¨ï¼Ÿ",
        "OpenCVæ˜¯ä»€ä¹ˆï¼Ÿ"
    ]
    
    for query in text_queries:
        print(f"\næŸ¥è¯¢: {query}")
        results = multimodal_system.text_search(query, k=2)
        
        for result in results:
            print(f"  ç»“æœ {result['rank']}:")
            print(f"    å†…å®¹: {result['content']}")
            print(f"    æ¥æº: {result['metadata'].get('source', 'æœªçŸ¥')}")
    
    # 5. æµ‹è¯•å›¾åƒå¤„ç†
    print("\n" + "=" * 60)
    print("æµ‹è¯•å›¾åƒå¤„ç†")
    print("=" * 60)
    
    if os.path.exists(image_file):
        image_questions = [
            "æè¿°è¿™å¼ å›¾ç‰‡",
            "å›¾ç‰‡ä¸­æœ‰ä»€ä¹ˆæ–‡å­—ï¼Ÿ",
            "è¿™å¼ å›¾ç‰‡ä¸è®¡ç®—æœºè§†è§‰æœ‰ä»€ä¹ˆå…³ç³»ï¼Ÿ"
        ]
        
        for question in image_questions:
            print(f"\né—®é¢˜: {question}")
            response = multimodal_system.process_image(image_file, question)
            print(f"å›ç­”: {response}")
    
    # 6. æµ‹è¯•å¤šæ¨¡æ€æŸ¥è¯¢
    print("\n" + "=" * 60)
    print("æµ‹è¯•å¤šæ¨¡æ€æŸ¥è¯¢")
    print("=" * 60)
    
    multimodal_queries = [
        ("è®¡ç®—æœºè§†è§‰æœ‰å“ªäº›åº”ç”¨ï¼Ÿ", image_file),
        ("å¦‚ä½•åˆ†æå›¾åƒï¼Ÿ", image_file),
        ("äººå·¥æ™ºèƒ½åœ¨å›¾åƒå¤„ç†ä¸­çš„ä½œç”¨", None)  # ä»…æ–‡æœ¬æŸ¥è¯¢
    ]
    
    for text_query, img_file in multimodal_queries:
        print(f"\nå¤šæ¨¡æ€æŸ¥è¯¢: {text_query}")
        if img_file:
            print(f"å›¾åƒæ–‡ä»¶: {img_file}")
        
        result = multimodal_system.multimodal_query(text_query, img_file)
        
        if result["combined_response"]:
            print(f"ç»¼åˆå“åº”:\n{result['combined_response']}")
        elif result["text_results"]:
            print("æ–‡æœ¬æœç´¢ç»“æœ:")
            for text_result in result["text_results"]:
                print(f"  - {text_result['content'][:100]}...")
        
        print("-" * 40)
    
    # 7. æ¸…ç†æ–‡ä»¶
    print("\næ¸…ç†ç¤ºä¾‹æ–‡ä»¶...")
    for file_path in [text_file, image_file]:
        if os.path.exists(file_path):
            os.remove(file_path)
            print(f"å·²åˆ é™¤: {file_path}")
    
    print("\nç¤ºä¾‹å®Œæˆï¼")

if __name__ == "__main__":
    main()